# ğŸš— Road Accidents Severity Prediction - France

## ğŸ¯ Project Goal

This MLOps project demonstrates a **production-grade machine learning pipeline** for predicting the severity of traffic accidents in France. The system helps emergency services and traffic management authorities prioritize response efforts by predicting accident outcomes based on environmental, temporal, and participant characteristics.

**Key Objectives:**
- Build an end-to-end MLOps pipeline with orchestration, monitoring, and serving capabilities
- Predict accident severity (Unscathed, Light injury, Hospitalized, Killed) from 16 features
- Implement model versioning, drift detection, and automated retraining workflows
- Provide a production-ready REST API with JWT authentication and Prometheus metrics

## ğŸ“Š Dataset

**Source**: [Accidents in France from 2005 to 2016](https://www.kaggle.com/datasets/ahmedlahlou/accidents-in-france-from-2005-to-2016/data) (Kaggle)

**Description**: Historical road accident data from the French government containing detailed information about accidents, vehicles, and users involved. The dataset includes:
- **Features**: 16 columns including temporal (year, month, hour), environmental (weather, luminosity, road type), and demographic factors (age, sex, user category)
- **Target**: Severity with 4 classes (1=Unscathed, 2=Light injury, 3=Hospitalized, 4=Killed)
- **Size**: 11+ years of accident records
- **Split**: 60% train, 20% validation, 20% test (stratified sampling)

## â†ªï¸ Architecture Overview

<!-- Mermaid.js script copied from the most up to date version in src/utils/schemas.py -->
```mermaid

%% %%{init: {"flowchart": {"curve": "curve"}}}%%
%% Choose between curve, linear, step, cardinal
%% default: curve

flowchart LR

    classDef transp fill:transparent,stroke:transparent;
    classDef user fill:#ff6f00,stroke:#b34700,stroke-width:3px,color:#ffffff;
    classDef airflow fill:#ffd54f,stroke:#b28704,stroke-width:3px,color:#000000;
    classDef api fill:#00c853,stroke:#007e33,stroke-width:3px,color:#ffffff;
    classDef db fill:#2962ff,stroke:#0039cb,stroke-width:3px,color:#ffffff;
    classDef mlflow fill:#00b0ff,stroke:#007bb2,stroke-width:3px,color:#ffffff;

    linkStyle default stroke:#000999,stroke-width:2px

    subgraph UA[USER APP]
        IFS[INTERACTIVE FEATURE INPUT]:::user
    end

    IFS --> |Features Input|EP



    subgraph MLF[MLFLOW]
        SR[STORE RUN]:::mlflow
        PLMS[PROD & LAST MODEL SCORE]:::mlflow
        UT[UPDATE TAGS]:::mlflow
        IPM[IDENTIFY PROD. MODEL]:::mlflow
    end



    IPM --> |Prod. Model|EP
    PLMS --> |Metrics|DPM





    subgraph API[MODEL API]
        ET[ENDPOINT /train]:::api
        EP[ENDPOINT /predict]:::api
    end


    EP --> |Query Prod. Model|IPM
    EP --> |Prediction|IFS
    ET -->|Data Query|FPD
    ET -->|Last Model & Metrics|SR



    subgraph DB[DATABASE]
        SRD[STORE RAW DATA]:::db
        SPD[STORE PREPROCESSED DATA]:::db
        FPD[FETCH PREPROCESSED DATA]:::db
    end

    FPD -->|Preprocessed Data|ET



    subgraph CAF[AIRFLOW]
        START[PROCESS START]:::airflow
        ETL[ETL]:::airflow
        TE[TRAIN & EVALUATE]:::airflow
        DPM[IDENTIFY PROD. MODEL]:::airflow
        END[PROCESS END<br>/ LOOP TO ETL]:::airflow
    end

    START --> ETL --> TE --> DPM --> END
    DPM -->|IF last model is better: Update Query|UT
    DPM --> |Last & Prod. Model Score Query|PLMS
    ETL --> |csv Files Query|DD
    ETL --> |Raw Data|SRD
    ETL --> |Preprocessed Data|SPD
    TE --> |Train New Model Query|ET
    linkStyle 8 stroke:#BA8E23,stroke-width:6px,stroke-dasharray:5 5
    linkStyle 9 stroke:#BA8E23,stroke-width:6px,stroke-dasharray:5 5
    linkStyle 10 stroke:#BA8E23,stroke-width:6px,stroke-dasharray:5 5
    linkStyle 11 stroke:#BA8E23,stroke-width:6px,stroke-dasharray:5 5

    

    subgraph KG[KAGGLE]
        DD[DATASET DOWNLOAD]:::db
    end

    DD --> |Raw csv Files|ETL



    subgraph LG[LEGEND]
        direction LR
        PROCESS
        D1[ ]:::transp
        D2[ ]:::transp
        D3[ ]:::transp
        D4[ ]:::transp
        D1 --> |DATA| D2
        D3 --> |PROCESS STEPS| D4
        linkStyle 20 stroke:#BA8E23,stroke-width:6px,stroke-dasharray:5 5
    end

```


## ğŸ—‚ï¸ Project Organization

The project is structured as follows:
```plaintext
.
â”œâ”€â”€ README.md                  <- Project overview and setup instructions
â”œâ”€â”€ docker-compose.yml         <- Multi-container orchestration (8 services)
â”œâ”€â”€ Dockerfile.*               <- Service-specific container definitions
â”œâ”€â”€ Makefile                   <- Common development commands (up, down, build, test)
â”œâ”€â”€ pyproject.toml             <- Python dependencies and project metadata
â”œâ”€â”€ requirements-*.txt         <- Service-specific Python requirements
â”‚
â”œâ”€â”€ .env                       <- Environment variables (create from .env.example)
â”œâ”€â”€ .github/                   <- GitHub configuration and AI agent instructions
â”‚
â”œâ”€â”€ airflow/                   <- Airflow-specific files
â”‚   â”œâ”€â”€ config/                <- Airflow configuration (airflow.cfg)
â”‚   â”œâ”€â”€ logs/                  <- DAG execution logs and reports
â”‚   â””â”€â”€ plugins/               <- Custom Airflow plugins
â”‚
â”œâ”€â”€ configs/                   <- Service configurations
â”‚   â”œâ”€â”€ grafana/               <- Grafana dashboards and provisioning
â”‚   â””â”€â”€ prometheus/            <- Prometheus scraping configuration
â”‚
â”œâ”€â”€ dags/                      <- Airflow DAG definitions
â”‚   â”œâ”€â”€ accidents_data_dag.py  <- ETL pipeline for data ingestion
â”‚   â”œâ”€â”€ accidents_ml_dag.py    <- ML training and evaluation pipeline
â”‚   â””â”€â”€ accidents_dag.py       <- Combined ETL + ML orchestration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   <- Original data from Kaggle (timestamped)
â”‚   â””â”€â”€ clean/                 <- Processed data ready for modeling
â”‚
â”œâ”€â”€ models/                    <- Saved model artifacts (if not in MLflow)
â”œâ”€â”€ notebooks/                 <- Jupyter notebooks for exploration
â”œâ”€â”€ references/                <- Data dictionaries and documentation
â”œâ”€â”€ scripts/                   <- Utility scripts (DB seeding, entrypoints)
â”‚
â”œâ”€â”€ src/                       <- Source code for the project
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api/                   <- FastAPI application
â”‚   â”‚   â””â”€â”€ main.py            <- REST endpoints (/predict, /health, /metrics)
â”‚   â”œâ”€â”€ auth/                  <- JWT authentication and user management
â”‚   â”œâ”€â”€ data/                  <- Data processing scripts
â”‚   â”‚   â”œâ”€â”€ download_data.py   <- Kaggle dataset download
â”‚   â”‚   â”œâ”€â”€ clean_data.py      <- Data cleaning and merging
â”‚   â”‚   â””â”€â”€ ingest_data.py     <- Database ingestion with progress tracking
â”‚   â”œâ”€â”€ features/              <- Feature engineering (currently unused)
â”‚   â”œâ”€â”€ models/                <- Model training and prediction
â”‚   â”‚   â”œâ”€â”€ train_model.py     <- Training with GridSearchCV
â”‚   â”‚   â”œâ”€â”€ predict_model.py   <- Prediction logic and model loading
â”‚   â”‚   â””â”€â”€ metrics.py         <- Custom metrics and evaluation
â”‚   â”œâ”€â”€ monitoring/            <- Drift detection and explainability
â”‚   â”‚   â”œâ”€â”€ drift.py           <- Evidently-based drift detection
â”‚   â”‚   â”œâ”€â”€ drift_reporter.py  <- Report generation and storage
â”‚   â”‚   â””â”€â”€ explainability.py  <- SHAP values and model interpretation
â”‚   â”œâ”€â”€ streamlit/             <- User-facing Streamlit application
â”‚   â”‚   â”œâ”€â”€ Home.py            <- Main entry point
â”‚   â”‚   â””â”€â”€ pages/             <- Multi-page app components
â”‚   â””â”€â”€ utils/                 <- Shared utilities
â”‚       â”œâ”€â”€ ml_utils.py        <- Central ML config and constants
â”‚       â”œâ”€â”€ database.py        <- DB connection and progress tracking
â”‚       â”œâ”€â”€ logging.py         <- Logging configuration
â”‚       â””â”€â”€ schemas.py         <- Pydantic models and data schemas
â”‚
â””â”€â”€ tests/                     <- Test suite
    â””â”€â”€ unit/                  <- Unit tests mirroring src/ structure
        â”œâ”€â”€ api/
        â”œâ”€â”€ data/
        â”œâ”€â”€ models/
        â”œâ”€â”€ monitoring/
        â””â”€â”€ utils/
```

## ğŸ›« Prerequisites

> everything that has to be done once before starting development.

0. install [UV](https://docs.astral.sh/uv/getting-started/installation/)
1. install python and its dependencies:
   ```bash
   uv sync
   ```
2. install [Docker](https://docs.docker.com/get-docker/).
3. create `.env` from the `.env.example` file and adapt values if needed

## ğŸš€ Quick Start

Get the entire MLOps pipeline running in 3 commands:

```bash
uv sync && source .venv/bin/activate  # Install dependencies
cp .env.example .env                  # Configure environment
make build && make up                 # Start all services
```

**Access Services:**
| Service | URL | Default Credentials |
|---------|-----|---------------------|
| Airflow UI | http://localhost:8080 | `airflow` / `airflow` |
| MLflow Tracking | http://localhost:5001 | - |
| FastAPI Docs | http://localhost:8000/docs | JWT token required |
| Streamlit UI | http://localhost:8501 | - |
| Grafana | http://localhost:3000 | `admin` / `admin` |
| MinIO Console | http://localhost:9000 | `mini_user` / `mini_password` |

## âŒ¨ï¸ Development Setup

> Do this every time you start working on the project.

1. Sync dependencies and activate virtual environment:
   ```bash
   uv sync
   source .venv/bin/activate
   ```
2. Start ğŸ³ Docker containers:
   ```bash
   make build  # Build images with BuildKit
   make up     # Start containers in detached mode
   ```
3. Verify all services are healthy:
   ```bash
   docker ps  # Check container status
   make logs  # Follow container logs
   ```


## ğŸª Airflow DAGs

The project includes the following Airflow DAGs for orchestrating workflows:

- [accidents data dag](./dags/accidents_data_dag.py): Manages chunked or full data ingestion. This allows for simulating data evolution over time.
- [accidents ml dag](./dags/accidents_ml_dag.py): Handles the machine learning pipeline, including data cleaning, dataset splitting, model training and evaluation.
- [accidents dag](./dags/accidents_dag.py): Orchestrates the ETL pipeline for data ingestion, cleaning, and model training.


## ğŸ“Š Model Training Details

### ğŸ§ª Validation Strategy

- **Static train/validation/test splits**: 60% / 20% / 20%
- **Stratified sampling**: Ensures balanced class distribution
- **Database-tracked**: Split assignments stored in `clean_data.dataset_split` column
- **Reproducible**: Fixed random seed (42) for consistent splits

### ğŸ§® Model & Metrics

- **Model**: Random Forest Classifier with 100 trees
- **Metrics**: Accuracy, Precision, Recall, F1-score (weighted), ROC-AUC
- **Artifacts**: Model, metrics, feature importance, confusion matrix, config

## ğŸ–¥ï¸ Local Development (Without Docker)

For faster iteration during development:

**1. Start the API locally:**
```bash
uvicorn src.api.main:app --reload
```

**2. Start the Streamlit App:**
```bash
PYTHONPATH=. streamlit run src/streamlit/Home.py
```

**3. Access at:**
- API: http://localhost:8000/docs
- Streamlit: http://localhost:8501

**Note**: Ensure PostgreSQL, MLflow, and MinIO containers are running for database and model access.

## ğŸ“š Additional Resources

- **Architecture Diagram**: See mermaid flowchart above for data flow
- **API Documentation**: Interactive docs at `/docs` endpoint (FastAPI auto-generated)
- **Model Training**: See `src/models/train_model.py` for standalone training script
- **DAG Documentation**: Detailed docstrings in `dags/*.py` files