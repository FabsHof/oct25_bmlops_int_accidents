# ğŸš— Road accidents in France

The objective of this MLOps project is to build a MLOps pipeline with the aim of predicting the severity of road accidents in France. Predictions will be based on historical data.

## ğŸ—‚ï¸ Project Organization

The project is structured as follows:
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ external       <- Data from third party sources.
    â”‚Â Â  â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ logs               <- Logs from training and predicting
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    â”‚                         the creator's initials, and a short `-` delimited description, e.g.
    â”‚                         `1.0-jqp-initial-data-exploration`.
    â”‚
    â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
    â”‚
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ __init__.py    <- Makes src a Python module
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ data           <- Scripts to download or generate data
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
    â”‚   â”‚                     predictions
    â”‚   â”‚
    â”‚Â Â  â””â”€â”€ config         <- Describe the parameters used in train_model.py and predict_model.py

## âš™ï¸ Setup

1. Install Python 3.11 or higher.
2. Install `uv` package manager from [uv package manager](https://uv.dev/).
3. Set up an environment variables file `.env` in the root directory (e.g., see `.env.example`).
3. Create a virtual environment:
   ```bash
   uv venv create .venv
   ```
4. Install dependencies:
   ```bash
   uv sync
   ```
5. Create and update environment variables in `.env` file as needed (see `.env.example`).

## âŒ¨ï¸ Development

1. Activate the environment:
   ```bash
   source .venv/bin/activate
   ```
2. Run the FastAPI application:
   ```bash
   make api_dev
    ```
3. Access the API documentation at `http://localhost:8000/docs` (find e.g. the API key in the `.env` file, set it via the "Authorize" button in the Swagger UI).

## ğŸ“Š Data Ingestion

This project supports two modes of data ingestion:

### Full Batch Loading
Load all data at once using the traditional ETL process:
```bash
make do_etl
```

### Chunked/Incremental Loading
Load data in chunks to simulate data evolution over time. This is useful for testing incremental model training and monitoring data arrival patterns.

- **Using Makefile:**
```bash
make ingest_data_chunked
```
- **Using the script directly:**
```bash
python -m src.data.ingest_data --mode chunked --chunk-size 10000
```
- **Using the API:**

1. Start the API server:
   ```bash
   make api_dev
   ```

2. Check ingestion progress:
   ```bash
   curl "http://localhost:8000/data/progress?api_key=YOUR_API_KEY"
   ```

3. Load the next chunk of data:
   ```bash
   curl -X POST "http://localhost:8000/data/ingest-chunk?api_key=YOUR_API_KEY"
   ```