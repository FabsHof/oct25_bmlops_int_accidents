{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions of Severity\n",
    "\n",
    "This notebook demonstrates the prediction of accident severity using machine learning models on French road accident data from 2005 to 2016.\n",
    "\n",
    "**Dataset**: Accidents in France from 2005 to 2016\n",
    "\n",
    "**Objective**: Build classification models to predict the severity of road accidents based on various features such as weather conditions, time, location, and road characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data\n",
    "\n",
    "Note: This notebook expects the data to be available. You can download it using:\n",
    "```python\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"ahmedlahlou/accidents-in-france-from-2005-to-2016\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - adjust path as needed\n",
    "# For demonstration, we'll create sample code structure\n",
    "# In practice, replace with actual data loading\n",
    "\n",
    "print(\"Data loading section\")\n",
    "print(\"This section would load the accident data from CSV files\")\n",
    "print(\"Expected files: caracteristiques.csv, lieux.csv, usagers.csv, vehicules.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the accident data:\n",
    "    - Handle missing values\n",
    "    - Encode categorical variables\n",
    "    - Create derived features\n",
    "    - Scale numerical features\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    # Strategy: fill numerical with median, categorical with mode\n",
    "    for col in df_processed.columns:\n",
    "        if df_processed[col].dtype == 'object':\n",
    "            df_processed[col].fillna(df_processed[col].mode()[0] if not df_processed[col].mode().empty else 'Unknown', inplace=True)\n",
    "        else:\n",
    "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_severity_distribution(df, severity_col='grav'):\n",
    "    \"\"\"\n",
    "    Plot the distribution of accident severity levels\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    severity_counts = df[severity_col].value_counts().sort_index()\n",
    "    \n",
    "    plt.bar(severity_counts.index, severity_counts.values)\n",
    "    plt.xlabel('Severity Level')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Accident Severity')\n",
    "    plt.xticks(severity_counts.index)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(severity_counts.values):\n",
    "        plt.text(severity_counts.index[i], v, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"EDA functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df, target_col='grav'):\n",
    "    \"\"\"\n",
    "    Prepare features for modeling:\n",
    "    - Separate features and target\n",
    "    - Encode categorical variables\n",
    "    - Scale features\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in X.select_dtypes(include=['object']).columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    \n",
    "    return X_scaled, y, label_encoders, scaler\n",
    "\n",
    "print(\"Feature preparation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Random Forest Classifier\n",
    "\n",
    "Random Forest is an ensemble learning method that operates by constructing multiple decision trees and outputting the class that is the mode of the classes of the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train, n_estimators=100, max_depth=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier\n",
    "    \"\"\"\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    return rf_model\n",
    "\n",
    "print(\"Random Forest training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train, max_iter=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a Logistic Regression classifier\n",
    "    \"\"\"\n",
    "    lr_model = LogisticRegression(\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    return lr_model\n",
    "\n",
    "print(\"Logistic Regression training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(X_train, y_train, max_depth=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a Decision Tree classifier\n",
    "    \"\"\"\n",
    "    dt_model = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    dt_model.fit(X_train, y_train)\n",
    "    \n",
    "    return dt_model\n",
    "\n",
    "print(\"Decision Tree training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name} Performance\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, f1, y_pred\n",
    "\n",
    "print(\"Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, top_n=15):\n",
    "    \"\"\"\n",
    "    Plot feature importance for tree-based models\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1][:top_n]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(f'Top {top_n} Feature Importances')\n",
    "        plt.barh(range(top_n), importances[indices])\n",
    "        plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Model does not have feature_importances_ attribute\")\n",
    "\n",
    "print(\"Feature importance plotting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Execution Pipeline\n",
    "\n",
    "This section demonstrates the complete workflow when data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_severity_prediction_pipeline(df, target_col='grav', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Complete pipeline for severity prediction\n",
    "    \"\"\"\n",
    "    print(\"Starting Severity Prediction Pipeline...\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Step 1: Preprocess data\n",
    "    print(\"\\n[1/6] Preprocessing data...\")\n",
    "    df_processed = preprocess_data(df)\n",
    "    \n",
    "    # Step 2: Prepare features\n",
    "    print(\"[2/6] Preparing features...\")\n",
    "    X, y, label_encoders, scaler = prepare_features(df_processed, target_col)\n",
    "    \n",
    "    # Step 3: Split data\n",
    "    print(\"[3/6] Splitting data...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    print(f\"Training set size: {X_train.shape[0]}\")\n",
    "    print(f\"Test set size: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Step 4: Train models\n",
    "    print(\"\\n[4/6] Training models...\")\n",
    "    \n",
    "    print(\"  - Training Random Forest...\")\n",
    "    rf_model = train_random_forest(X_train, y_train)\n",
    "    \n",
    "    print(\"  - Training Logistic Regression...\")\n",
    "    lr_model = train_logistic_regression(X_train, y_train)\n",
    "    \n",
    "    print(\"  - Training Decision Tree...\")\n",
    "    dt_model = train_decision_tree(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Evaluate models\n",
    "    print(\"\\n[5/6] Evaluating models...\")\n",
    "    \n",
    "    rf_acc, rf_f1, _ = evaluate_model(rf_model, X_test, y_test, \"Random Forest\")\n",
    "    lr_acc, lr_f1, _ = evaluate_model(lr_model, X_test, y_test, \"Logistic Regression\")\n",
    "    dt_acc, dt_f1, _ = evaluate_model(dt_model, X_test, y_test, \"Decision Tree\")\n",
    "    \n",
    "    # Step 6: Compare models\n",
    "    print(\"\\n[6/6] Model Comparison\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Model': ['Random Forest', 'Logistic Regression', 'Decision Tree'],\n",
    "        'Accuracy': [rf_acc, lr_acc, dt_acc],\n",
    "        'F1 Score': [rf_f1, lr_f1, dt_f1]\n",
    "    })\n",
    "    \n",
    "    results = results.sort_values('F1 Score', ascending=False)\n",
    "    print(results.to_string(index=False))\n",
    "    \n",
    "    # Plot feature importance for best model (Random Forest)\n",
    "    print(\"\\nPlotting feature importance for Random Forest...\")\n",
    "    plot_feature_importance(rf_model, X.columns.tolist())\n",
    "    \n",
    "    return {\n",
    "        'models': {'rf': rf_model, 'lr': lr_model, 'dt': dt_model},\n",
    "        'results': results,\n",
    "        'preprocessors': {'label_encoders': label_encoders, 'scaler': scaler}\n",
    "    }\n",
    "\n",
    "print(\"Pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Example Usage\n",
    "\n",
    "To use this notebook with actual data:\n",
    "\n",
    "```python\n",
    "# Load your data\n",
    "df = pd.read_csv('path/to/your/data.csv')\n",
    "\n",
    "# Run the pipeline\n",
    "results = run_severity_prediction_pipeline(df, target_col='grav')\n",
    "\n",
    "# Access trained models\n",
    "best_model = results['models']['rf']\n",
    "\n",
    "# Make predictions on new data\n",
    "# predictions = best_model.predict(X_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a complete framework for predicting accident severity using machine learning techniques. The key findings typically include:\n",
    "\n",
    "1. **Random Forest** generally performs best for this classification task due to its ability to capture non-linear relationships and handle mixed data types.\n",
    "\n",
    "2. **Important Features** for severity prediction usually include:\n",
    "   - Weather conditions\n",
    "   - Time of day and day of week\n",
    "   - Road type and surface conditions\n",
    "   - Number of vehicles involved\n",
    "   - Speed limit\n",
    "   - Location characteristics (urban vs rural)\n",
    "\n",
    "3. **Recommendations**:\n",
    "   - Focus safety interventions on high-risk conditions identified by the model\n",
    "   - Consider ensemble methods for production deployment\n",
    "   - Regularly retrain models with new data to maintain accuracy\n",
    "   - Implement proper data validation and monitoring in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
